<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<title>PITCHME.html</title>

</head>

<body>

<hr />

<h1>Perf, SQL, etc.</h1>

<h3>Lunch &amp; Learn (Apr 21, 2017)</h3>

<h5>viyer</h5>

<hr />

<h2>Overview</h2>

<ul>
<li>Diagnosing perf problems</li>
<li>ActiveRecord -> EXPLAIN (SQL internals)</li>
<li>Bonus 1 (Redshift)</li>
<li>Bonus 2 (Hive)</li>
</ul>

<!-- Caveats about I am not an expert -->

<hr />

<h2>Where in the stack is the perf bottleneck?</h2>

<p><img src="diagrams/perf-stack.png" alt="perf-stack" title="" /></p>

<hr />

<h2>The optimization loop</h2>

<p><img src="diagrams/opt-loop.png" alt="opt-loop" title="" /></p>

<hr />

<h2>Profilers</h2>

<p><img src="diagrams/profilers.png" alt="profilers" title="" /></p>

<hr />

<p>üêª</p>

<hr />

<h2>NewRelic</h2>

<p>+++</p>

<h3>Apdex = % satisfactory response times</h3>

<ul>
<li>satisfying &lt; tolerable (T) &lt; frustrating (4T)</li>
<li>Global setting of T = 300ms</li>
<li>Response time setting to record "traces" >= 4T</li>
<li>Can be changed on per-transaction level (for key transactions)</li>
</ul>

<p>+++</p>

<h3>Custom instrumentation</h3>

<p>Set or modify transaction names:</p>

<p><code>ruby
  def contacts_results(search_term, tags, page_number: 0, paginate: true)
    transaction_name = NewRelic::Agent.get_transaction_name
    if search_term.blank?
      NewRelic::Agent.set_transaction_name("#{transaction_name} - empty")
    elsif UserConfig.value_for_user_id_and_key(@user.id, 'experimental-search').to_b
      NewRelic::Agent.set_transaction_name("#{transaction_name} - experimental")
    elsif search_term.split(/\s+/).length &gt; 3
      NewRelic::Agent.set_transaction_name("#{transaction_name} - long")
    end
</code>
+++</p>

<p>Measure a block:</p>

<p>```ruby
   include ::NewRelic::Agent::MethodTracer</p>

<p>self.trace<em>execution</em>scoped(['MessageThreadTagsController/log<em>work</em>action']) do
     log<em>work</em>action(:message<em>thread</em>tagged, message<em>thread</em>tag, client<em>created</em>at: nil, thread<em>id: params[:message</em>thread<em>id].to</em>i)
   end
```</p>

<p>+++</p>

<p>Split out a method:</p>

<p><code>ruby
add_method_tracer :contacts_results, 'ClientSearch/contacts_results'
</code></p>

<p>+++</p>

<p>[notes]</p>

<ul>
<li>transactions</li>
<li>traces</li>
<li>database</li>
<li>dashboards</li>
<li>throughput v. slow queries</li>
<li>percentiles</li>
</ul>

<hr />

<h2>Profiling Ruby</h2>

<p>+++</p>

<p><code>!debug</code></p>

<ul>
<li>Turn log level to DEBUG (shows Elasticsearch queries)</li>
<li>Pipe the ActiveRecord query logs to STDOUT</li>
<li>shows backtraces etc</li>
</ul>

<p>[demo]</p>

<hr />

<p><a href="https://github.com/finventures/fin-core-beta/blob/master/.pryrc"><code>.pryrc</code></a></p>

<p>Custom commands:</p>

<ul>
<li>debugger</li>
<li><code>dp</code> | <code>ep</code></li>
<li>backtrace</li>
<li>autocompleter</li>
<li>prompt</li>
</ul>

<hr />

<p><code>Benchmark</code></p>

<p>```ruby</p>

<p>rails> Benchmark.bm do |x|
*   x.report { User.find(2) }
* end
  user     system      total        real
  0.020000   0.010000   0.030000 (  0.031872)</p>

<p>rails> Benchmark.realtime { User.find 2 }
=> 0.004197215981548652</p>

<p>```</p>

<p>+++</p>

<p><code>RubyProf</code></p>

<p>```ruby</p>

<blockquote>
  <p>result = RubyProf.profile { AgentState.event_loop }
printer = RubyProf::CallStackPrinter.new(result)
printer.print(open("./profile2.html", "w"))</p>
</blockquote>

<p>$> open profile2.html</p>

<p>```</p>

<hr />

<p>üåù</p>

<hr />

<h1>SQL</h1>

<ul>
<li>Declarative (v. imperative). cf. <code>select, map, group_by</code> v. <code>for</code> loops</li>
<li><strong>Structured English Query Language</strong> (<code>SEQUEL</code>) -> <code>SQL</code></li>
<li>Ingres(s) (Berkeley) -> Postgres (1980's)</li>
</ul>

<p>+++</p>

<p><a href="http://www.morganslibrary.net/files/codd-1970.pdf">EF Codd, a Relational Algebra</a></p>

<h3>Relation</h3>

<blockquote>
Given sets X1 , S1, S2, . . , S, (not necessarily
distinct), R is a relation on these n sets if it is a set of ntuples
each of which has its first element from S1, its
second element from Sz , and so on.
</blockquote>

<p>+++</p>

<h2>What is ACID-compliance?</h2>

<ul>
<li>Atomicity: Transaction that rolls back completely or commits completely</li>
<li>Consistency: Transaction rolls back on trigger failure or validation violation</li>
<li>Isolation: Concurrent transactions</li>
<li>Durability: Write-ahead log</li>
</ul>

<hr />

<h3>MVCC</h3>

<ul>
<li>Every transaction has an id (<code>xid</code>)</li>
<li>Every row can have multiple versions, each has an <code>(xmin, xmax)</code></li>
<li>Rows are only visible for read to <code>xid IN [xmin, xmax]</code></li>
<li><code>INSERT</code>: create version with <code>xmin = xid</code></li>
<li><code>DELETE</code>: create version with <code>xmax = xid</code></li>
<li><code>COMMIT</code>: set <code>committed[xid] = true</code></li>
<li>Always read latest visible committed version</li>
</ul>

<p>+++</p>

<h2>Rollbacks</h2>

<ul>
<li>Can leave versions lying around, bloating the db and indexes</li>
<li><code>VACUUM</code> cleans these up</li>
<li>Postgres has <code>auto_vacuum</code>. Tuning frequency of vacuuming (&amp; analyzing) is a
big deal for perf</li>
</ul>

<p>+++</p>

<h2>xid Wraparound</h2>

<ul>
<li><code>xid</code> is 32-bit integer (2^32 = 4 billion)</li>
<li>If it wraps around -> 0, all rows have <code>xmin</code> > <code>xid</code> and become visible üí•</li>
<li>Need to <code>VACUUM FREEZE</code> to set <code>xmin</code> to sentinel value</li>
</ul>

<hr />

<h2>Write-ahead logs</h2>

<ul>
<li>Every <code>COMMIT</code>, write a binary log of the changes made to data</li>
<li><p>Regularly <strong>checkpoint</strong>, ie. flush to durable storage</p></li>
<li><p>Crash recovery: find last checkpoint and replay</p></li>
<li>Replication: ship binary log over and replay</li>
</ul>

<p>+++</p>

<h2>Log-table duality</h2>

<blockquote>
Logs are tables in motion -- Jay Kreps
</blockquote>

<p>cf. Streams (Kafka/Kinesis etc.)</p>

<hr />

<p>üê°</p>

<hr />

<h3><code>ActiveRecord</code>, <code>arel</code> etc</h3>

<p>(Pat Shaughnessy)</p>

<p><img src="diagrams/arel-1.png" alt="arel-1" title="" /></p>

<p>+++</p>

<p><img src="diagrams/arel-2.png" alt="arel-2" title="" /></p>

<p>+++</p>

<p><img src="diagrams/arel-3.png" alt="arel-3" title="" /></p>

<hr />

<p>üëæ</p>

<hr />

<h3>The Database</h3>

<p>+++</p>

<p><img src="diagrams/libpq.png" alt="libpq" title="" /></p>

<p>+++</p>

<h3>Parser ‚Üí Rewriter ‚Üí Optimizer ‚Üí Cost analysis ‚Üí Execute</h3>

<p>+++</p>

<p><img src="diagrams/parser.png" alt="parser" title="" /></p>

<p>+++</p>

<h3>Rewriter</h3>

<p>Replace Views with underlying query tree</p>

<p>+++</p>

<h3>Optimizer</h3>

<ul>
<li>How to scan tables
<ul>
<li>e.g. Sequential, Index</li>
</ul></li>
<li>How to join tables (and in what order)
<ul>
<li>e.g. Hash, Merge</li>
</ul></li>
<li>Inlining, predicate pushdown etc</li>
</ul>

<p>üìå</p>

<p>+++</p>

<p><img src="diagrams/query-plan.png" alt="plan" title="" /></p>

<p>+++</p>

<h3>Cost analysis across many possible plans</h3>

<p><code>postgresql.conf</code></p>

<p>```
seq<em>page</em>cost = 1.0                    # Read 8K Page from disk sequentially
random<em>page</em>cost = 4.0                 # Random I/O
cpu<em>tuple</em>cost = 0.01                  # Process a row
cpu<em>index</em>tuple<em>cost = 0.005           # Process an index entry
cpu</em>operator_cost = 0.0025             # Perform an operation</p>

<p>```</p>

<p>Goal: Find a good plan fast enough so that it actually matters</p>

<p>+++</p>

<h3>Statistics</h3>

<ul>
<li>Estimate # of rows returned by query (each step in plan)</li>
<li><code>ANALYZE</code> updates statistics</li>
<li><code>auto_vacuum</code> runs <code>ANALYZE</code>
<ul>
<li><code>n_distinct</code>,</li>
<li><code>most_common_vals</code> (and their frequencies),</li>
<li><code>histogram_bounds</code> (distribution of values)</li>
</ul></li>
</ul>

<p>üëé  Assumes independence of columns</p>

<p>+++</p>

<h3>Executor</h3>

<ul>
<li>stream-processing style dependency graph</li>
<li>Parent node <em>pulls</em> on its children</li>
<li>i.e. time to first row matters, as does generating all rows</li>
</ul>

<hr />

<p>ü•ë</p>

<hr />

<h2>Tables and Indexes</h2>

<p><img src="diagrams/index-table.png" alt="index-table" title="" /></p>

<hr />

<h2>Reading a table</h2>

<ul>
<li>Cost depends on how much data you're reading</li>
<li>Index reads are random and expensive (but you can read only what you want)</li>
<li>Sequential reads are cheap (but you have to read all of it)</li>
</ul>

<p>+++</p>

<h2>Sequential scan</h2>

<p>(when <em>selectivity</em> is low)</p>

<p><img src="diagrams/seq-scan.png" alt="seq-scan" title="" /></p>

<p>+++</p>

<p>```sql
explain analyze
select id from users where id is not null;</p>

<p>Seq Scan on users  (cost=0.00..992.41 rows=3241 width=4)
  Filter: (id IS NOT NULL)
```</p>

<p>+++</p>

<h2>Index scan</h2>

<p>(when <em>selectivity</em> is high)</p>

<p><img src="diagrams/index-scan.png" alt="index-scan" title="" /></p>

<p>+++</p>

<p>```sql
explain
select id from users where id=2;</p>

<p>Index Only Scan using users_pkey on users  (cost=0.28..8.30 rows=1 width=4)
  Index Cond: (id = 2)</p>

<p>```</p>

<p>+++</p>

<p>üç©  Index-<em>only</em> scans: when <strong>all</strong> columns involved are indexed</p>

<p>```sql
explain
select * from users where id=2;</p>

<p>Index Scan using users_pkey on users  (cost=0.28..8.30 rows=1 width=1713)
  Index Cond: (id = 2)
```</p>

<p>+++</p>

<h2>Bitmap Index Scan + Bitmap Heap Scan</h2>

<p>(when <em>selectivity</em> is intermediate)</p>

<p><img src="diagrams/bitmap-scan.png" alt="bitmap-scan" title="" /></p>

<p>+++</p>

<p>```sql</p>

<p>explain
select * from users where id > 20 and id &lt; 50;</p>

<p>Bitmap Heap Scan on users  (cost=4.53..89.50 rows=24 width=1713)
  Recheck Cond: ((id > 20) AND (id &lt; 50))
    ->  Bitmap Index Scan on users_pkey  (cost=0.00..4.52 rows=24 width=0)
            Index Cond: ((id > 20) AND (id &lt; 50))</p>

<p>```</p>

<h2>üç´  can combine multiple indices</h2>

<p>üî¨</p>

<hr />

<h2><code>EXPLAIN</code></h2>

<ul>
<li><code>EXPLAIN</code>: plan + estimated costs</li>
<li><code>EXPLAIN ANALYZE</code>: plan + estimated costs + <em>actual</em> costs after execution</li>
<li><code>EXPLAIN (ANALYZE, COSTS, VERBOSE, BUFFERS, FORMAT JSON)</code> : more stats about
how much data was read etc.</li>
</ul>

<p>+++</p>

<p>```</p>

<p>(cost=4.53..89.50 rows=24 width=1713)</p>

<p>(actual time=0.007..0.133 rows=100 loops=1)</p>

<p>```</p>

<ul>
<li>starting cost / actual starting time</li>
<li>total cost / actual total time</li>
<li>rows</li>
<li>width</li>
<li>loops</li>
</ul>

<p>+++</p>

<h3>Control row estimates through statistics target</h3>

<p><code>
show default_statistics_target
100
</code></p>

<p>```
SELECT attname, attstattarget
FROM   pg<em>attribute
WHERE  attrelid = 'entry</em>properties'::regclass;</p>

<hr />

<p>entry<em>id    10000
schema</em>id    10000
serialized_value    10000
...</p>

<p>```</p>

<p><code>ALTER TABLE SET STATISTICS</code></p>

<hr />

<h2>Joins üôè</h2>

<p>+++</p>

<h3>Nested loop join with Sequential scan</h3>

<p><img src="diagrams/nested-loop-join.png" alt="nested-loop-join" title="" /></p>

<p>(small tables)</p>

<p>+++</p>

<h3>Nested loop join with Index scan</h3>

<p><img src="diagrams/nested-loop-index-join.png" alt="nested-loop-index-join" title="" /></p>

<p>+++</p>

<h3>Hash join</h3>

<p><img src="diagrams/hash-join.png" alt="hash-join" title="" /></p>

<p>+++</p>

<h3>Merge join</h3>

<p><img src="diagrams/merge-join.png" alt="merge-join" title="" /></p>

<hr />

<h3>Other operations</h3>

<ul>
<li>Sort (<code>ORDER BY</code>)</li>
<li>Hash Aggregate (<code>GROUP BY</code>)</li>
<li>CTEs (<code>WITH</code>) -- üò≤  <em>optimization fences</em></li>
<li>Materialize (save loops)</li>
</ul>

<hr />

<h3>Experimenting with different strategies</h3>

<p><code>
enable_bitmapscan = on
enable_hashagg = on
enable_hashjoin = on
enable_indexscan = on
enable_indexonlyscan = on
enable_material = on
enable_mergejoin = on
enable_nestloop = on
enable_seqscan = on
enable_sort = on
enable_tidscan = on
</code></p>

<hr />

<h2>Optimization in practice</h2>

<ul>
<li>Measure, profile</li>
<li>Batch things (reads &amp; writes, round trips are expensive)</li>
<li>Do things once (no n+1s)</li>
<li>Precompute it</li>
<li>Cache it</li>
<li>Understand why it doesn't work as you expect</li>
<li>Add the right indices.</li>
<li>Experiment. Measure.</li>
<li>set statistics (maybe other settings, like memory?), <code>ANALYZE</code>, <code>VACUUM</code></li>
<li>Work around the problem cases</li>
<li>Buy bigger boxes</li>
</ul>

<hr />

<p>üöó</p>

<hr />

<h2>Bonus 1: Redshift</h2>

<ul>
<li>No indexes</li>
<li>Sharded (data distributed over multiple nodes)</li>
</ul>

<p>+++</p>

<h3>Principles</h3>

<ul>
<li>Moving data is bad (bring the compute to the data)</li>
<li>Filtering data is good</li>
<li>Distributing computation is good</li>
</ul>

<p>+++</p>

<h3><code>DISTKEY</code> &amp; <code>DISTSTYLE</code></h3>

<ul>
<li>How to shard
<ul>
<li>EVEN</li>
<li>ALL</li>
<li>KEY (set <code>DISTKEY</code>)</li>
</ul></li>
<li><code>JOIN</code> keys should be <code>diststyle=KEY</code></li>
<li>Small dimension tables should be <code>ALL</code></li>
<li>distribution should not be skewed</li>
</ul>

<p>+++</p>

<h3><code>SORTKEY</code></h3>

<ul>
<li>Filter keys should be <code>SORTKEY</code></li>
</ul>

<hr />

<p>üêù</p>

<hr />

<h2>Bonus 2: SQL over Map Reduce</h2>

<p>(eg. Hive, Presto)</p>

<p>+++</p>

<h2>MapReduce (eg. Hadoop)</h2>

<ul>
<li>Data is sharded across compute nodes (lives on a <em>distributed</em> file system)</li>
<li>Compute nodes can behave as <code>mapper</code>s or <code>reducers</code></li>
</ul>

<p>+++</p>

<h3>Principles</h3>

<ul>
<li>Moving data is bad (bring the compute to the data)</li>
<li>Filtering data is good</li>
<li>Distributing computation is good</li>
</ul>

<p>+++</p>

<h3>Steps</h3>

<ol>
<li>Map phase</li>
<li>Hash partition + Local sort</li>
<li>Distribute</li>
<li>Merge sort</li>
<li>Reduce</li>
</ol>

<p>+++</p>

<h3>Map (Mapper node)</h3>

<ul>
<li>Read the shard of data on the current node</li>
<li>Perform an operation on each row, one at a time</li>
<li>Emit the row</li>
</ul>

<p>+++</p>

<h3>Hash Partition + Local Sort (Mapper node)</h3>

<ul>
<li>Hash partition (ie. shard) the output across Reducer nodes</li>
<li>Sort each partition</li>
</ul>

<h3>Distribute + Merge sort (Reducer node)</h3>

<ul>
<li>Pull your shards from all the mappers</li>
<li>Merge sort</li>
<li>Apply <code>reduce()</code> to each group (already sorted)</li>
</ul>

<p>+++</p>

<p><img src="diagrams/mapreduce-join.png" alt="mapreduce-join" title="" /></p>

<p>+++</p>

<h3>SQL -> Mapreduce</h3>

<ul>
<li><code>WHERE</code> = map</li>
<li><code>ORDER BY</code> = sort</li>
<li><code>GROUP BY</code> = reduce</li>
<li><code>JOIN</code> = hash partition on join keys and merge join</li>
</ul>

<hr />

</body>
</html>
