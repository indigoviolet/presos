<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<title>PITCHME.html</title>

</head>

<body>

<hr />

<h1>Perf, SQL, etc.</h1>

<h3>Lunch &amp; Learn Apr 20, 2017</h3>

<h5>Venky Iyer</h5>

<hr />

<h2>Overview</h2>

<p>+++</p>

<ul>
<li>Identifying perf problems</li>
<li>Montage: SQL operations/internals</li>
<li>What affects SQL query performance</li>
<li>EXPLAIN output</li>
<li>Redshift SQL &amp; Hive SQL</li>
</ul>

<!-- Caveats about I am not an expert -->

<hr />

<h2>Where in the stack is the perf bottleneck?</h2>

<p><img src="diagrams/perf-stack.png" alt="perf-stack" title="" /></p>

<hr />

<h2>The optimization loop</h2>

<p><img src="diagrams/opt-loop.png" alt="opt-loop" title="" /></p>

<hr />

<h2>Profilers</h2>

<p><img src="diagrams/profilers.png" alt="profilers" title="" /></p>

<hr />

<p>üêª</p>

<hr />

<h2>NewRelic</h2>

<p>+++</p>

<h3>Apdex = % satisfactory response times</h3>

<ul>
<li>satisfying &lt; tolerable (T) &lt; frustrating (4T)</li>
<li>Global setting of T = 300ms</li>
<li>Response time setting to record "traces" >= 4T</li>
<li>Can be changed on per-transaction level (for key transactions)</li>
</ul>

<p>+++</p>

<h3>Custom instrumentation</h3>

<p>Set or modify transaction names:</p>

<p><code>ruby
  def contacts_results(search_term, tags, page_number: 0, paginate: true)
    transaction_name = NewRelic::Agent.get_transaction_name
    if search_term.blank?
      NewRelic::Agent.set_transaction_name("#{transaction_name} - empty")
    elsif UserConfig.value_for_user_id_and_key(@user.id, 'experimental-search').to_b
      NewRelic::Agent.set_transaction_name("#{transaction_name} - experimental")
    elsif search_term.split(/\s+/).length &gt; 3
      NewRelic::Agent.set_transaction_name("#{transaction_name} - long")
    end
</code>
+++</p>

<p>Measure a block:</p>

<p>```ruby
   include ::NewRelic::Agent::MethodTracer</p>

<p>self.trace<em>execution</em>scoped(['MessageThreadTagsController/log<em>work</em>action']) do
     log<em>work</em>action(:message<em>thread</em>tagged, message<em>thread</em>tag, client<em>created</em>at: nil, thread<em>id: params[:message</em>thread<em>id].to</em>i)
   end
```</p>

<p>+++</p>

<p>Split out a method:</p>

<p><code>ruby
add_method_tracer :contacts_results, 'ClientSearch/contacts_results'
</code></p>

<p>+++</p>

<p>[notes]</p>

<ul>
<li>transactions</li>
<li>traces</li>
<li>database</li>
<li>dashboards</li>
<li>throughput v. slow queries</li>
<li>percentiles</li>
</ul>

<hr />

<h2>Profiling Ruby</h2>

<p>+++</p>

<p><code>!debug</code></p>

<ul>
<li>Turn log level to DEBUG (shows Elasticsearch queries)</li>
<li>Pipe the ActiveRecord query logs to STDOUT</li>
<li>shows backtraces etc</li>
</ul>

<p>[demo]</p>

<hr />

<p><a href="https://github.com/finventures/fin-core-beta/blob/master/.pryrc"><code>.pryrc</code></a></p>

<p>Custom commands:</p>

<ul>
<li>debugger</li>
<li><code>dp</code> | <code>ep</code></li>
<li>backtrace</li>
<li>autocompleter</li>
<li>prompt</li>
</ul>

<hr />

<p><code>Benchmark</code></p>

<p>```ruby</p>

<p>rails> Benchmark.bm do |x|
*   x.report { User.find(2) }
* end
  user     system      total        real
  0.020000   0.010000   0.030000 (  0.031872)</p>

<p>rails> Benchmark.realtime { User.find 2 }
=> 0.004197215981548652</p>

<p>```</p>

<p>+++</p>

<p><code>RubyProf</code></p>

<p>```ruby</p>

<blockquote>
  <p>result = RubyProf.profile { AgentState.event_loop }
printer = RubyProf::CallStackPrinter.new(result)
printer.print(open("./profile2.html", "w"))</p>
</blockquote>

<p>$> open profile2.html</p>

<p>```</p>

<hr />

<p>üåù</p>

<hr />

<h1>SQL</h1>

<ul>
<li>Declarative (v. imperative). cf. <code>select, map, group_by</code> v. <code>for</code> loops</li>
<li><strong>Structured English Query Language</strong> (<code>SEQUEL</code>) -> <code>SQL</code></li>
<li>Ingres(s) (Berkeley) -> Postgres (1980's)</li>
</ul>

<p>+++</p>

<p><a href="http://www.morganslibrary.net/files/codd-1970.pdf">EF Codd, a Relational Algebra</a></p>

<h3>Relation</h3>

<blockquote>
Given sets X1 , S1, S2, . . , S, (not necessarily
distinct), R is a relation on these n sets if it is a set of ntuples
each of which has its first element from S1, its
second element from Sz , and so on.
</blockquote>

<p>+++</p>

<h2>What is ACID-compliance?</h2>

<ul>
<li>Atomicity: Transaction that rolls back completely or commits completely</li>
<li>Consistency: Transaction rolls back on trigger failure or validation violation</li>
<li>Isolation: Concurrent transactions</li>
<li>Durability: Write-ahead log</li>
</ul>

<hr />

<h3>MVCC</h3>

<ul>
<li>Every transaction has an id (<code>xid</code>)</li>
<li>Every row can have multiple versions, each has an <code>(xmin, xmax)</code></li>
<li>Rows are only visible for read to <code>xid IN [xmin, xmax]</code></li>
<li><code>INSERT</code>: create version with <code>xmin = xid</code></li>
<li><code>DELETE</code>: create version with <code>xmax = xid</code></li>
<li><code>COMMIT</code>: set <code>committed[xid] = true</code></li>
<li>Always read latest visible committed version</li>
</ul>

<p>+++</p>

<h2>Rollbacks</h2>

<ul>
<li>Can leave versions lying around, bloating the db and indexes</li>
<li><code>VACUUM</code> cleans these up</li>
<li>Postgres has <code>auto_vacuum</code>. Tuning this is a big deal for perf</li>
</ul>

<p>+++</p>

<h2>xid Wraparound</h2>

<ul>
<li><code>xid</code> is 32-bit integer (2^32 = 4 billion)</li>
<li>If it wraps around -> 0, all rows have <code>xmin</code> > <code>xid</code> and become visible üí•</li>
<li>Need to <code>VACUUM FREEZE</code> to set <code>xmin</code> to sentinel value</li>
</ul>

<hr />

<h2>Write-ahead logs</h2>

<ul>
<li>Every <code>COMMIT</code>, write a binary log of the changes made to data</li>
<li><p>Regularly <strong>checkpoint</strong>, ie. flush to durable storage</p></li>
<li><p>Crash recovery: find last checkpoint and replay</p></li>
<li>Replication: ship binary log over and replay</li>
</ul>

<p>+++</p>

<h2>Log-table duality</h2>

<blockquote>
Logs are tables in motion -- Jay Kreps
</blockquote>

<p>cf. Streams (Kafka/Kinesis etc.)</p>

<hr />

<p>üê°</p>

<hr />

<h3><code>ActiveRecord</code>, <code>arel</code> etc</h3>

<p>(Pat Shaughnessy)</p>

<p><img src="diagrams/arel-1.png" alt="arel-1" title="" /></p>

<p>+++</p>

<p><img src="diagrams/arel-2.png" alt="arel-2" title="" /></p>

<p>+++</p>

<p><img src="diagrams/arel-3.png" alt="arel-3" title="" /></p>

<hr />

<p>üëæ</p>

<hr />

<h3>The Database</h3>

<p>+++</p>

<p><img src="diagrams/libpq.png" alt="libpq" title="" /></p>

<p>+++</p>

<h3>Parser ‚Üí Rewriter ‚Üí Optimizer ‚Üí Cost analysis ‚Üí Execute</h3>

<p>+++</p>

<p><img src="diagrams/parser.png" alt="parser" title="" /></p>

<p>+++</p>

<h3>Rewriter</h3>

<p>Replace Views with underlying query tree</p>

<p>+++</p>

<h3>Optimizer</h3>

<ul>
<li>How to scan tables</li>
<li>How to join tables (and in what order)</li>
<li>Inlining, predicate pushdown etc</li>
</ul>

<p>üìå</p>

<p>+++</p>

<h3>Cost analysis</h3>

<p><code>postgresql.conf</code></p>

<p>```
seq<em>page</em>cost = 1.0                    # Read 8K Page from disk sequentially
random<em>page</em>cost = 4.0                 # Random I/O
cpu<em>tuple</em>cost = 0.01                  # Process a row
cpu<em>index</em>tuple<em>cost = 0.005           # Process an index entry
cpu</em>operator_cost = 0.0025             # Perform an operation</p>

<p>```</p>

<p>Goal: Find a good path fast enough so that it actually matters</p>

<p>+++</p>

<p><img src="diagrams/query-plan.png" alt="plan" title="" /></p>

<p>+++</p>

<h3>Executor</h3>

<ul>
<li>stream-processing style dependency graph</li>
<li>Parent node <em>pulls</em> on its children</li>
<li>i.e. time to first row matters, as does generating all rows</li>
</ul>

<hr />

</body>
</html>
